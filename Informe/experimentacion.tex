\chapter{Entorno de experimentación}

\section{Ejemplos seleccionados}

% Descripción de los corpus.
	% Etiquetado
		% Las semillas son muy importantes. Deberían ser elegidas al azar?
		% La muestra debería ser representativa de la población? Aproximadamente un 10% de las preguntas son reconocidas por quepy, deberíamos incluir esto en el set de entrenamiento?
	% No etiquetado
	% Testing
		% Las que reconoce quepy?
		% 500 preguntas está bien?

Distribución actual del corpus:
Quepy questions 115
	Recognized 58
	Unrecognized 57
Other questions 6658
	Labeled 607
    Unlabeled 6051

Test corpus 250
Training corpus 165
Unlabeled corpus 6358

\section{Usuarios Emulados}

Para poder realizar experimentos automáticos sin que el usuario tenga que ingresar la misma información repretidas veces decidimos guardar las respuestas obtenidas. Por ello agregamos etiquetas al corpus no etiquetados, que por supuesto no son consultadas, y creamos un corpus de asociaciones características-clases.
El corpus de características no es más que una matriz ternaria con tres valores posibles: asociación positiva, no asociación, desconocido. De esta forma también evitamos preguntar a un usuario por las características que ya han sido vistas pero no están relacionadas a ninguna clase en particular. Observemos que esta forma de guardar la información soporta etiquetas múltiples.

\section{Experimentos realizados}

% Automáticos sin usuarios
% Con usuarios


\subsection{Métricas utilizadas}
\begin{description}
    \item[Exactitud] Llamamos exactitud a la cantidad de preguntas etiquetadas correctamente sobre el total de preguntas clasificadas.
    %Precisión se refiere a la dispersión del conjunto de valores obtenidos de mediciones repetidas de una magnitud (wikipedia)
    \item[Curva de aprendizaje] Definimos la curva de aprendizaje como la exactitud del clasificador en función de la cantidad de ejemplos o características etiquetados necesarios.
    % aprendizaje sobre instancias
    % sobre features
    % sobre ambos
    % sobre ninguno
    \item [Coeficiente Kappa de Cohen] Esta medida ajusta la exactitud del clasificador obtenido a la de un clasificador aleatorio. Una exactitud del 80\% no es muy sorprendente si asignando etiquetas al azar obtenemos una exactitud del 70\%. En nuestro caso el corpus de evaluación contiene aproximadamente un 75\% de instacias de clase ``otro'', por lo tanto un clasificador que elija esta etiqueta todas las veces obtendría una exactitud semejante. Esta métrica nos permitirá superar este inconveniente y obtener valores más reales.\\
    Una definición más formal del Coeficiente de Kappa es la que propone \citet{KappaCarletta}:
    $$K = \frac{P(A)-P(E)}{1-P(E)}$$
    donde $P(A)$ es la proporción de veces que los clasificadores acuerdan y $P(E)$ es la proporción de veces que se esperaría que acuerden por casualidad. En este caso, uno de los clasificadores es el Multinomial Bayesiano entrenado y el otro son las etiquetas del corpus de evaluación. Por lo tanto, $P(A)$ no es otra cosa más que la exactitud calculada en el primer item. Adicionalmente, calculamos $P(E)$ de la siguiente forma:
    $$P(E) = \frac{\sum_{i\in\mathcal{C}}Pr(\hat{x_i})*Pr(x_i)}{|\mathcal{E}|}$$
    donde $\mathcal{C}$ es el conjunton de clases, $\mathcal{E}$ es el corpus de evaluación, $Pr(\hat{x_i})$ es la proporción de instancias etiquetadas por el clasificador con la clase i, y $Pr(x_i)$ es la proporción de instancias que pertenecen realmente a la clase i.
    % http://stats.stackexchange.com/questions/82162/kappa-statistic-in-plain-english
    \item [Precisión y exhaustividad por clase] Estas dos medidas puede utilizarse sólo en clasificación binaria, por lo que tomaremos sus valores para cada una de las clases posibles. Definimos precisión como la cantidad de instancias etiquetadas para una clase que son correctas (positivos verdaderos o $P_v$) sobre la cantidad de instancias etiquetadas para esa clase ($P_v$ y falsos positivos o $P_f$).
    $$Precision(C_i) = \frac{P_v}{P_v + P_f}$$
    La exhaustividad, por otro lado, está definida como la cantidad de instancias etiquetadas correctamente ($P_v$) de una clase dada sobre la cantidad de instancias que pertenecen a la clase verdaderamente ($P_v$ y falsos negativos o $N_f$).
    $$Exhaustividad(C_i) = \frac{P_v}{P_v + N_f}$$
\end{description}

\subsection{Baseline}
Tomaremos como baseline dos métodos:
\begin{description}
    \item[Selección de instancias y características al azar]
    % \item[Bootstrapqing normal utilizando EM]  % Vale la pena hacer esto?
\end{description}

Los experimentos que solicitan al oráculo información sobre instancias y características se realizaron alternando pocas preguntas relativas a instancias y la misma cantidad relativas a características. Si bien los dos ciclos de etiquetado son completamente independientes en el sistema, tomamos esta desición para eliminar las diferencias entre experimentos que puedan introducirse a partir de la elección de los usuarios. Otra punto en el que difieren nuestros experimentos de una sesión no simulada con un usuario es la cantidad de veces que se reentrena. Con objeto de obtener una medición precisa de la curva de aprendizaje reentrenamos el clasificador luego de cada ronda intancias-características descriptas anteriormente. Esto es
costoso para grandes volúmenes de datos y podría hacer perder al sistema su interactividad.

\subsection{Hipótesis 1}
\textbf{El aprendizaje activo obtiene mejores resultados que un clasificador normal utilizando la misma cantidad de datos.}

Para comprobar esta hipótesis realizamos sesiones de etiquetamiento sobre instancias y características, pero eligiendo al azar cuáles presentar al usuario. Luego compararemos las curvas de aprendizaje de ambos experimentos.

\subsection{Hipótesis 2}
\textbf{El aprendizaje activo sobre instancias y características obtiene mejores resultados que el aprendizaje activo sobre instancias o características por separado.}



\subsection{Experimento 3}
\textbf{Hipótesis} El aprendizaje supervisado sobre instancias y características obtiene mejores resultados que el aprendizaje supervisado sobre instancias, aún si las características son pocas.


\subsection{Experimento 4}
\textbf{Hipótesis} ??.

La idea es ver qué método de selección de features es mejor:\\

Puede ser trabajo futuro



\textbf{Resultados}\\

\begin{tabular}{||p{4cm} | l | l||}
\hline
 & Clase con probabilidad alta & Clase con probabilidad baja \\
\hline
Característica con probabilidad alta & medicion & medicion \\
\hline
Característica con probabilidad baja & medicion & medicion \\
\hline
\end{tabular}
\hfill
\begin{tabular}{||p{4cm} | l | l||}
\hline
 & Clase con probabilidad alta & Clase con probabilidad baja \\
\hline
Característica con IG alta & medicion & medicion \\
\hline
Característica con IG baja & medicion & medicion \\
\hline

\end{tabular}



\subsection{Experimento 5}
\textbf{Hipótesis} Seleccionar features para etiquetar que tengan alta confiabilidad/correlación, y luego de superado un cierto límite pasar a los que tiene baja confiabilidad/correlación permite al clasificador eliminar el ruido no introducido por la baja cantidad de ejemplos y al mismo tiempo expandir la cobertura.
Dejar para mas adelante


Experimento Entrenamiento supervisado con y sin etiquetado de  features
Validacion del etiquetado de features



Experimento 6
Information gain sobre todo el corpus o solo el etiquetado.
IG sobre el corpus anotado + frecuencia en no anotado vs IG sobre todo el corpus anotado y no anotado.


Experimento 7
Coocurrencia de features con otros features. (Información mutua)
Un feature se rankea mas alto si coocurre con features que se rankean alto. Tomando como base la frecuencia.

Experimento 8
Information gain sirve o alcanza sólo con usar coocurrencia?
