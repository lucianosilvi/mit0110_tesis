\chapter{Entorno de experimentación}

\section{Ejemplos seleccionados}

% Descripción de los corpus.
	% Etiquetado
		% Las semillas son muy importantes. Deberían ser elegidas al azar?
		% La muestra debería ser representativa de la población? Aproximadamente un 10% de las preguntas son reconocidas por quepy, deberíamos incluir esto en el set de entrenamiento?
	% No etiquetado
	% Testing
		% Las que reconoce quepy?
		% 500 preguntas está bien?

Distribución actual del corpus:
Quepy questions 115
	Recognized 58
	Unrecognized 57
Other questions 6658
	Labeled 607
	Unlabeled 6051

Test corpus 250
Training corpus 165
Unlabeled corpus 6358

\section{Experimentos realizados}

% Automáticos sin usuarios
% Con usuarios


\subsection{Métricas utilizadas}
\begin{description}
    \item[Precisión] Mediremos cuántas preguntas del corpus de testing fueron etiquetadas correctamente por el clasificador, sobre el total de preguntas clasificadas.
    \item[Curva de aprendizaje] Definimos la curva de aprendizaje como la precisión del clasificador en función de la cantidad de ejemplos o características etiquetados necesarios.
    \item Factor kappa  -- Puede ser complejo
    \item [Precisión y recall por clase] Definimos precisión como 
    % Agregar definiciones
\end{description}

\subsection{Baseline}
Tomaremos como baseline dos métodos:
\begin{description}
    \item[Selección de instancias y características al azar]
    \item[Bootstrapqing normal utilizando EM]  % Vale la pena hacer esto?
\end{description}

Los experimentos que solicitan al oráculo información sobre instancias y características se realizaron alternando pocas preguntas relativas a instancias y la misma cantidad relativas a características. Si bien los dos ciclos de etiquetado son completamente independientes en el sistema, tomamos esta desición para eliminar las diferencias entre experimentos que puedan introducirse a partir de la elección de los usuarios. Otra punto en el que difieren nuestros experimentos de una sesión no simulada con un usuario es la cantidad de veces que se reentrena. Con objeto de obtener una medición precisa de la curva de aprendizaje reentrenamos el clasificador luego de cada ronda intancias-características descriptas anteriormente. Esto es
costoso para grandes volúmenes de datos y podría hacer perder al sistema su interactividad.

\subsection{Experimento 1}
\textbf{Hipótesis} El aprendizaje activo obtiene mejores resultados que un clasificador normal utilizando la misma cantidad de datos.


aprendizaje sobre instancias
sobre features
sobre ambos
sobre ninguno

\subsection{Experimento 2}
\textbf{Hipótesis} El aprendizaje activo sobre instancias y características obtiene mejores resultados que el aprendizaje activo sobre instancias o características por separado.
% Cómo lo medimos? Por tiempo? Por cantidad de datos etiquedos?
% Por cantidad de acciones que realiza un usuario?
% Probablemente termine siendo el mismo experimento que el anterior

\subsection{Experimento 3}
\textbf{Hipótesis} El aprendizaje supervisado sobre instancias y características obtiene mejores resultados que el aprendizaje supervisado sobre instancias, aún si las características son pocas.


\subsection{Experimento 4}
\textbf{Hipótesis} ??.

La idea es ver qué método de selección de features es mejor:\\

Puede ser trabajo futuro



\textbf{Resultados}\\

\begin{tabular}{||p{4cm} | l | l||}
\hline
 & Clase con probabilidad alta & Clase con probabilidad baja \\
\hline
Característica con probabilidad alta & medicion & medicion \\
\hline
Característica con probabilidad baja & medicion & medicion \\
\hline
\end{tabular}
\hfill
\begin{tabular}{||p{4cm} | l | l||}
\hline
 & Clase con probabilidad alta & Clase con probabilidad baja \\
\hline
Característica con IG alta & medicion & medicion \\
\hline
Característica con IG baja & medicion & medicion \\
\hline

\end{tabular}



\subsection{Experimento 5}
\textbf{Hipótesis} Seleccionar features para etiquetar que tengan alta confiabilidad/correlación, y luego de superado un cierto límite pasar a los que tiene baja confiabilidad/correlación permite al clasificador eliminar el ruido no introducido por la baja cantidad de ejemplos y al mismo tiempo expandir la cobertura.
Dejar para mas adelante


Experimento Entrenamiento supervisado con y sin etiquetado de  features
Validacion del etiquetado de features



Experimento 6
Information gain sobre todo el corpus o solo el etiquetado.
IG sobre el corpus anotado + frecuencia en no anotado vs IG sobre todo el corpus anotado y no anotado.


Experimento 7
Coocurrencia de features con otros features. (Información mutua)
Un feature se rankea mas alto si coocurre con features que se rankean alto. Tomando como base la frecuencia.

Experimento 8
Information gain sirve o alcanza sólo con usar coocurrencia?
