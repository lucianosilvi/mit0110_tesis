\section{Arquitectura del sistema}

Si bien el objetivo principal de este trabajo es incrementar la cobertura de Quepy, deseamos también que el sistema esté compartimentado de tal forma que sus componentes puedan utilizarse individualmente para otras aplicaciones.

Trabajamos sobre dos grandes áreas: un clasificador automático entrenado utlizando aprendizaje activo sobre instancias y características, y la representación de las preguntas para maximizar los resultados del clasificador. El clasificador puede ser visto como una tarea general, y por lo tanto lo desarrollamos como una librería portable cuyos parámetros pueden ser definidos por el usuario. Sin embargo, la representación de las preguntas elegidas es algo completamente ligado al sistema de preguntas y por ello lo diseñamos como una extensión opcional de Quepy que puede utilizar cualquier clasificador.

A continuación detallaremos cada una de estos módulos, y finalmente abordaremos la estructura del sistema completo.

\subsection{Framework para aprendizaje automático sobre instancias y características}
% Basado en DUALIST Esto sería lo que haría conjuntamente con Rodri.
% Necesitamos pensar un nombre!
\subsubsection{Funcionalidad}
	% Para entrenarlo con active learning
	% Para obtener métricas (con esto lo vamos a medir)

\subsubsection{Parámetros}
\begin{description}
	\item[Clasificador] El usuario debe definir qué clasificador de sklearn utilizar.
	\item[Características]
		% Debe ser un algo que encaje dentro de un Pipeline
	\item[Corpus]
		% Qué archivos usar
		% Corpus training - no etiquetado - comprobación
			% El no etiquetado puede tener etiquetas para testing.
		% Formato que deben tener
			% diccionario de la forma {'question': pregunta, 'target': clase}
	\item[Función de representación al usuario] El usuario debe proveer una interfáz gráfica para presentar los datos al usuario y obtener una respuesta.
\end{description}
Si bien ésta aproximación es altamente paramétrica, cabe destacar que permite gran flexibilidad con respecto a los datos ingresados. Al permitir elegir tanto características como el corpus y el clasificador, puede ser utilizado dentro de cualquier ámbito incluso no relacionado al procesamiento del lenguaje natural.
% En particular se utilizaría para iepy, y citamos la tesis de Rodri. Eso se puede?

\subsection{Interfaz Quepy - Clasificador}
% Tiene que servir para otros framework que no incluyan active learning.
% Tiene que tener alguna forma de hacer testing, así podemos obtener los resultados que queremos.
% Tomamos como base el ejemplo de quepy freebase.

\subsection{Arquitectura general}

% Descripción del ciclo usuario - reentrenamiento - EM
\subsubsection{Selección de instancias}
\subsubsection{Selección de características}
Las características son ordenadas luego del entrenamiento por su ganancia de información, que corresponde a:
% fórmula IG
La ganancia de información es propuesta por numerosos estudios como una forma de seleccionar las características más relevantes para clasificación de texto.
% citar los textos que la mencionan.
Para lograr una clasificación más sencilla, se crea una lista de las características más probables para cada clase. Luego, estas características se ordenan de acuerto a su ganancia de información.

\subsubsection{Maximización de la esperanza}
El algoritmo de maximización de la esperanza es ampliamente utilizado para inferir parámetros desconocidos en una distribución que tiene estados latentes. Para esto, utiliza estimadores de máxima verosimilitud, y para clasificación el estado latente al que nos referimos es la misma clase. A grandes rasgos, el proceso funciona en dos pasos E y M, por sus siglas en inglés Expectation y Maximization.
El paso E es el que calcula el valor esperado de la verosimilitud asumiendo que la distribución actual es verdadera y no existen variables no observadas. Para realizarlo, utilizamos el clasificador en su estado actual para etiquetar probabilisticamente el conjunto de datos no etiquetados, y asumimos dichas etiquetas como correctas. Con este conjunto de nuevos datos se calcula la verosimilitud del modelo.
Recordemos que la verosimilitud es una función sobre los parámetros del modelo y un conjunto de datos que calcula la probabilidad de los valores tomados por cada variable aleatoria del modelo bajo dichos parámetros.
Si los parámetros actuales fueran correctos, entonces la verosimilitud obtenida sería la verosimilitud real del modelo, pero si no lo son provee una cota inferior. Luego, como tenemos una función sin valores no observados, el paso M maximiza la función encontrando parámetros del modelo más adecuados. De esta forma optimizamos la cota inferior encontrada.
Este paso se repite numerosas veces hasta lograr convengercia. Sin embargo tomamos ejemplo de Settles y realizamos una sola iteración, dado que argumenta que las siguientes iteraciones no aportan significativamente a la precisión del clasificador.

%http://ai.stanford.edu/~chuongdo/papers/em_tutorial.pdf

% http://stackoverflow.com/questions/11808074/what-is-an-intuitive-explanation-of-expectation-maximization-technique

% http://cs.brown.edu/courses/cs195-5/fall2009/docs/lecture_11-12.pdf

% http://cs229.stanford.edu/notes/cs229-notes8.pdf
